{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nK = keras.backend\n\n# Common imports\nimport numpy as np\nimport os\nimport timeit\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T15:29:18.939952Z","iopub.execute_input":"2021-06-09T15:29:18.940264Z","iopub.status.idle":"2021-06-09T15:29:23.688644Z","shell.execute_reply.started":"2021-06-09T15:29:18.940235Z","shell.execute_reply":"2021-06-09T15:29:23.687887Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# load data and split into train / validation sets\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\nX_train, y_train = X_train_full[:45000], y_train_full[:45000]\nX_valid, y_valid = X_train_full[45000:], y_train_full[45000:]","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-06-09T15:29:26.458425Z","iopub.execute_input":"2021-06-09T15:29:26.458759Z","iopub.status.idle":"2021-06-09T15:29:33.184380Z","shell.execute_reply.started":"2021-06-09T15:29:26.458727Z","shell.execute_reply":"2021-06-09T15:29:33.183603Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 4s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# initialize a sequential model (using 'stretch pants' approach)\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(1, 21):\n    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))\n\n# set optimizer and compile the model\noptimizer = keras.optimizers.Nadam(lr=5e-5)\nmodel.compile(loss='sparse_categorical_crossentropy',\n             optimizer=optimizer,\n             metrics=['accuracy'])\n\n# create callbacks\n# early stopping \ne_stop = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n# model checkpoint callback\ncheckpoint = keras.callbacks.ModelCheckpoint('cifar10_model.h5', save_best_only=True)\n# model logging\nrun_index = 1\nrun_log_dir = os.path.join(os.curdir, 'cifar_10_logs', 'run_{:03d}'.format(run_index))\n# tensorboard callback\ntensorboard = keras.callbacks.TensorBoard(run_log_dir)\n\ncallbacks = [e_stop, checkpoint]\n\n# fit model\nmodel.fit(X_train, y_train, epochs=100,\n         validation_data=(X_valid, y_valid),\n         callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T21:26:23.432527Z","iopub.status.idle":"2021-06-08T21:26:23.433498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.load_model(\"cifar10_model.h5\")\noptimizer = keras.optimizers.Nadam(lr=5e-5)\nmodel.compile(loss='sparse_categorical_crossentropy',\n             optimizer=optimizer,\n             metrics=['accuracy'])\nmodel.evaluate(X_valid, y_valid, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T18:11:08.895531Z","iopub.execute_input":"2021-06-08T18:11:08.895865Z","iopub.status.idle":"2021-06-08T18:11:09.831896Z","shell.execute_reply.started":"2021-06-08T18:11:08.895835Z","shell.execute_reply":"2021-06-08T18:11:09.830993Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[1.5092333555221558, 0.4681999981403351]"},"metadata":{}}]},{"cell_type":"code","source":"# clear keras session and reset randomizer\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# instantiate new sequential model using Batch Normalization layer\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nmodel.add(keras.layers.BatchNormalization())\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation('elu'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))\n\noptimizer = keras.optimizers.Nadam(lr=5e-4)\nmodel.compile(loss='sparse_categorical_crossentropy',\n             optimizer=optimizer,\n             metrics=['accuracy'])\n\n# update model checkpoint callback\ncheckpoint = keras.callbacks.ModelCheckpoint('cifar10_bn.h5', save_best_only=True)\ncallbacks = [e_stop, checkpoint]\n\nmodel.fit(X_train, y_train, epochs=100,\n         validation_data=(X_valid, y_valid),\n         callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T18:34:25.388875Z","iopub.execute_input":"2021-06-08T18:34:25.389192Z","iopub.status.idle":"2021-06-08T19:00:07.234305Z","shell.execute_reply.started":"2021-06-08T18:34:25.389162Z","shell.execute_reply":"2021-06-08T19:00:07.233505Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.8301 - accuracy: 0.3467 - val_loss: 1.7014 - val_accuracy: 0.3936\nEpoch 2/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.6597 - accuracy: 0.4077 - val_loss: 1.6009 - val_accuracy: 0.4274\nEpoch 3/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.5955 - accuracy: 0.4305 - val_loss: 1.5250 - val_accuracy: 0.4494\nEpoch 4/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.5509 - accuracy: 0.4466 - val_loss: 1.4598 - val_accuracy: 0.4910\nEpoch 5/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.5010 - accuracy: 0.4664 - val_loss: 1.4457 - val_accuracy: 0.4878\nEpoch 6/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 1.4696 - accuracy: 0.4777 - val_loss: 1.4156 - val_accuracy: 0.5020\nEpoch 7/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.4339 - accuracy: 0.4942 - val_loss: 1.3969 - val_accuracy: 0.5106\nEpoch 8/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.4069 - accuracy: 0.5034 - val_loss: 1.4033 - val_accuracy: 0.5014\nEpoch 9/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.3795 - accuracy: 0.5116 - val_loss: 1.3495 - val_accuracy: 0.5316\nEpoch 10/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.3643 - accuracy: 0.5174 - val_loss: 1.3679 - val_accuracy: 0.5142\nEpoch 11/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.3362 - accuracy: 0.5274 - val_loss: 1.3583 - val_accuracy: 0.5158\nEpoch 12/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.3198 - accuracy: 0.5318 - val_loss: 1.3503 - val_accuracy: 0.5214\nEpoch 13/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.2980 - accuracy: 0.5398 - val_loss: 1.3478 - val_accuracy: 0.5202\nEpoch 14/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.2806 - accuracy: 0.5450 - val_loss: 1.3446 - val_accuracy: 0.5280\nEpoch 15/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.2662 - accuracy: 0.5516 - val_loss: 1.3551 - val_accuracy: 0.5186\nEpoch 16/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.2425 - accuracy: 0.5608 - val_loss: 1.3394 - val_accuracy: 0.5232\nEpoch 17/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.2273 - accuracy: 0.5651 - val_loss: 1.3389 - val_accuracy: 0.5260\nEpoch 18/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.2107 - accuracy: 0.5725 - val_loss: 1.3346 - val_accuracy: 0.5296\nEpoch 19/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.1971 - accuracy: 0.5791 - val_loss: 1.3114 - val_accuracy: 0.5474\nEpoch 20/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.1904 - accuracy: 0.5818 - val_loss: 1.3284 - val_accuracy: 0.5386\nEpoch 21/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.1678 - accuracy: 0.5902 - val_loss: 1.3218 - val_accuracy: 0.5346\nEpoch 22/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 1.1666 - accuracy: 0.5877 - val_loss: 1.3221 - val_accuracy: 0.5376\nEpoch 23/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.1501 - accuracy: 0.5970 - val_loss: 1.3155 - val_accuracy: 0.5400\nEpoch 24/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.1373 - accuracy: 0.5951 - val_loss: 1.3446 - val_accuracy: 0.5360\nEpoch 25/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.1245 - accuracy: 0.6032 - val_loss: 1.3088 - val_accuracy: 0.5372\nEpoch 26/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 1.1140 - accuracy: 0.6091 - val_loss: 1.3321 - val_accuracy: 0.5388\nEpoch 27/100\n1407/1407 [==============================] - 31s 22ms/step - loss: 1.1043 - accuracy: 0.6114 - val_loss: 1.3273 - val_accuracy: 0.5338\nEpoch 28/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.0895 - accuracy: 0.6151 - val_loss: 1.3059 - val_accuracy: 0.5418\nEpoch 29/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.0805 - accuracy: 0.6201 - val_loss: 1.3212 - val_accuracy: 0.5462\nEpoch 30/100\n1407/1407 [==============================] - 33s 23ms/step - loss: 1.0707 - accuracy: 0.6242 - val_loss: 1.3681 - val_accuracy: 0.5366\nEpoch 31/100\n1407/1407 [==============================] - 33s 23ms/step - loss: 1.0601 - accuracy: 0.6271 - val_loss: 1.3381 - val_accuracy: 0.5374\nEpoch 32/100\n1407/1407 [==============================] - 33s 23ms/step - loss: 1.0505 - accuracy: 0.6280 - val_loss: 1.3479 - val_accuracy: 0.5394\nEpoch 33/100\n1407/1407 [==============================] - 33s 23ms/step - loss: 1.0485 - accuracy: 0.6336 - val_loss: 1.3467 - val_accuracy: 0.5366\nEpoch 34/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.0233 - accuracy: 0.6405 - val_loss: 1.3443 - val_accuracy: 0.5424\nEpoch 35/100\n1407/1407 [==============================] - 31s 22ms/step - loss: 1.0196 - accuracy: 0.6391 - val_loss: 1.3236 - val_accuracy: 0.5440\nEpoch 36/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 1.0079 - accuracy: 0.6460 - val_loss: 1.3409 - val_accuracy: 0.5420\nEpoch 37/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 0.9978 - accuracy: 0.6484 - val_loss: 1.3661 - val_accuracy: 0.5366\nEpoch 38/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 0.9907 - accuracy: 0.6514 - val_loss: 1.3601 - val_accuracy: 0.5408\nEpoch 39/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 0.9801 - accuracy: 0.6551 - val_loss: 1.3652 - val_accuracy: 0.5544\nEpoch 40/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 0.9738 - accuracy: 0.6566 - val_loss: 1.3689 - val_accuracy: 0.5422\nEpoch 41/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 0.9685 - accuracy: 0.6597 - val_loss: 1.3628 - val_accuracy: 0.5394\nEpoch 42/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 0.9556 - accuracy: 0.6631 - val_loss: 1.4074 - val_accuracy: 0.5304\nEpoch 43/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 0.9474 - accuracy: 0.6661 - val_loss: 1.3728 - val_accuracy: 0.5354\nEpoch 44/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 0.9360 - accuracy: 0.6716 - val_loss: 1.3895 - val_accuracy: 0.5408\nEpoch 45/100\n1407/1407 [==============================] - 32s 22ms/step - loss: 0.9354 - accuracy: 0.6720 - val_loss: 1.3813 - val_accuracy: 0.5436\nEpoch 46/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 0.9262 - accuracy: 0.6728 - val_loss: 1.3838 - val_accuracy: 0.5394\nEpoch 47/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 0.9178 - accuracy: 0.6783 - val_loss: 1.3848 - val_accuracy: 0.5426\nEpoch 48/100\n1407/1407 [==============================] - 32s 23ms/step - loss: 0.9102 - accuracy: 0.6798 - val_loss: 1.3663 - val_accuracy: 0.5378\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fddf7050490>"},"metadata":{}}]},{"cell_type":"code","source":"# load saved model, compile and validate\nmodel = keras.models.load_model('cifar10_bn.h5')\nmodel.compile(loss='sparse_categorical_crossentropy',\n             optimizer=optimizer,\n             metrics=['accuracy'])\nmodel.evaluate(X_valid, y_valid, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T19:00:50.307266Z","iopub.execute_input":"2021-06-08T19:00:50.307589Z","iopub.status.idle":"2021-06-08T19:00:52.602272Z","shell.execute_reply.started":"2021-06-08T19:00:50.307560Z","shell.execute_reply":"2021-06-08T19:00:52.601365Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[1.305877685546875, 0.5418000221252441]"},"metadata":{}}]},{"cell_type":"code","source":"# normalize inputs\nX_means = X_train.mean(axis=0)\nX_stds = X_train.std(axis=0)\nX_train_scaled = (X_train - X_means) / X_stds\nX_valid_scaled = (X_valid - X_means) / X_stds\nX_test_scaled = (X_test - X_means) / X_stds\n\nkeras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# self-normalizing model\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100,\n                                 kernel_initializer=\"lecun_normal\",\n                                 activation=\"selu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\noptimizer = keras.optimizers.Nadam(lr=5e-4)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\n\ncheckpoint = keras.callbacks.ModelCheckpoint(\"cifar10_selu.h5\", save_best_only=True)\nrun_index = 1 \nrun_logdir = os.path.join(os.curdir, \"cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\ncallbacks = [e_stop, checkpoint]\n\nmodel.fit(X_train_scaled, y_train, epochs=100,\n          validation_data=(X_valid_scaled, y_valid),\n          callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T19:07:27.088737Z","iopub.execute_input":"2021-06-08T19:07:27.089115Z","iopub.status.idle":"2021-06-08T19:14:25.785956Z","shell.execute_reply.started":"2021-06-08T19:07:27.089083Z","shell.execute_reply":"2021-06-08T19:14:25.785255Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.8861 - accuracy: 0.3240 - val_loss: 1.9271 - val_accuracy: 0.3440\nEpoch 2/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 1.6707 - accuracy: 0.4055 - val_loss: 1.6433 - val_accuracy: 0.4260\nEpoch 3/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.5721 - accuracy: 0.4440 - val_loss: 1.5915 - val_accuracy: 0.4376\nEpoch 4/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 1.4953 - accuracy: 0.4702 - val_loss: 1.5620 - val_accuracy: 0.4472\nEpoch 5/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 1.4365 - accuracy: 0.4954 - val_loss: 1.5159 - val_accuracy: 0.4688\nEpoch 6/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.3859 - accuracy: 0.5142 - val_loss: 1.4755 - val_accuracy: 0.4932\nEpoch 7/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 1.3382 - accuracy: 0.5315 - val_loss: 1.4663 - val_accuracy: 0.4872\nEpoch 8/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.3041 - accuracy: 0.5436 - val_loss: 1.4585 - val_accuracy: 0.4926\nEpoch 9/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 1.2639 - accuracy: 0.5633 - val_loss: 1.4576 - val_accuracy: 0.4968\nEpoch 10/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 1.2312 - accuracy: 0.5743 - val_loss: 1.4736 - val_accuracy: 0.4978\nEpoch 11/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.1927 - accuracy: 0.5885 - val_loss: 1.4413 - val_accuracy: 0.5068\nEpoch 12/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 1.1671 - accuracy: 0.5959 - val_loss: 1.4499 - val_accuracy: 0.5040\nEpoch 13/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.1327 - accuracy: 0.6142 - val_loss: 1.4713 - val_accuracy: 0.5032\nEpoch 14/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 1.1049 - accuracy: 0.6208 - val_loss: 1.4743 - val_accuracy: 0.5072\nEpoch 15/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 1.0810 - accuracy: 0.6293 - val_loss: 1.5151 - val_accuracy: 0.5044\nEpoch 16/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.0548 - accuracy: 0.6392 - val_loss: 1.4890 - val_accuracy: 0.5076\nEpoch 17/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 1.0328 - accuracy: 0.6483 - val_loss: 1.5089 - val_accuracy: 0.5162\nEpoch 18/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 1.0030 - accuracy: 0.6567 - val_loss: 1.5307 - val_accuracy: 0.5068\nEpoch 19/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 0.9856 - accuracy: 0.6609 - val_loss: 1.5189 - val_accuracy: 0.5266\nEpoch 20/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 0.9628 - accuracy: 0.6714 - val_loss: 1.5387 - val_accuracy: 0.5164\nEpoch 21/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 0.9428 - accuracy: 0.6801 - val_loss: 1.5446 - val_accuracy: 0.5172\nEpoch 22/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 0.9276 - accuracy: 0.6863 - val_loss: 1.5728 - val_accuracy: 0.5110\nEpoch 23/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 0.9059 - accuracy: 0.6946 - val_loss: 1.5637 - val_accuracy: 0.5038\nEpoch 24/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 0.8906 - accuracy: 0.7002 - val_loss: 1.6008 - val_accuracy: 0.4996\nEpoch 25/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 0.8681 - accuracy: 0.7066 - val_loss: 1.5745 - val_accuracy: 0.5114\nEpoch 26/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 0.8521 - accuracy: 0.7138 - val_loss: 1.5558 - val_accuracy: 0.5116\nEpoch 27/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 0.8397 - accuracy: 0.7174 - val_loss: 1.6909 - val_accuracy: 0.5060\nEpoch 28/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 0.8687 - accuracy: 0.7086 - val_loss: 1.5671 - val_accuracy: 0.5028\nEpoch 29/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 0.9743 - accuracy: 0.6813 - val_loss: 1.6375 - val_accuracy: 0.5174\nEpoch 30/100\n1407/1407 [==============================] - 13s 9ms/step - loss: 0.7991 - accuracy: 0.7309 - val_loss: 1.6703 - val_accuracy: 0.5132\nEpoch 31/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 0.7625 - accuracy: 0.7458 - val_loss: 1.6557 - val_accuracy: 0.5064\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fe200333250>"},"metadata":{}}]},{"cell_type":"code","source":"model = keras.models.load_model(\"cifar10_selu.h5\")\noptimizer = keras.optimizers.Nadam(lr=5e-4)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=optimizer,\n              metrics=[\"accuracy\"])\nmodel.evaluate(X_valid_scaled, y_valid)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T19:14:33.299774Z","iopub.execute_input":"2021-06-08T19:14:33.300099Z","iopub.status.idle":"2021-06-08T19:14:34.414858Z","shell.execute_reply.started":"2021-06-08T19:14:33.300069Z","shell.execute_reply":"2021-06-08T19:14:34.413998Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 0s 2ms/step - loss: 1.4413 - accuracy: 0.5068\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[1.441342830657959, 0.5067999958992004]"},"metadata":{}}]},{"cell_type":"code","source":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# add more regularization with AlphaDropout\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, \n                                 kernel_initializer='lecun_normal',\n                                activation='selu'))\nmodel.add(keras.layers.AlphaDropout(rate=0.2))\nmodel.add(keras.layers.Dense(10, activation='softmax'))\n\noptimizer = keras.optimizers.Nadam(lr=5e-4)\nmodel.compile(loss='sparse_categorical_crossentropy',\n             optimizer=optimizer,\n             metrics=['accuracy'])\n\ncheckpoint = keras.callbacks.ModelCheckpoint('cifar10_selu_alpha.h5', save_best_only=True)\nlog_idx = 1\nlog_dir = os.path.join(os.curdir, 'cifar_10_logs', 'run_alpha_{:03d}'.format(log_idx))\ntensorboard = keras.callbacks.TensorBoard(log_dir)\ncallbacks = [e_stop, checkpoint]\n\nmodel.fit(X_train_scaled, y_train, epochs=100,\n         validation_data=(X_valid_scaled, y_valid),\n         callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T19:18:37.444057Z","iopub.execute_input":"2021-06-08T19:18:37.444443Z","iopub.status.idle":"2021-06-08T19:25:09.505332Z","shell.execute_reply.started":"2021-06-08T19:18:37.444410Z","shell.execute_reply":"2021-06-08T19:25:09.504426Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.8878 - accuracy: 0.3275 - val_loss: 2.0161 - val_accuracy: 0.3716\nEpoch 2/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.6711 - accuracy: 0.4109 - val_loss: 1.6732 - val_accuracy: 0.4142\nEpoch 3/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.5867 - accuracy: 0.4391 - val_loss: 1.6626 - val_accuracy: 0.4338\nEpoch 4/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.5215 - accuracy: 0.4662 - val_loss: 1.6556 - val_accuracy: 0.4452\nEpoch 5/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.4645 - accuracy: 0.4885 - val_loss: 1.6429 - val_accuracy: 0.4616\nEpoch 6/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.4176 - accuracy: 0.5061 - val_loss: 1.5640 - val_accuracy: 0.4892\nEpoch 7/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.3772 - accuracy: 0.5232 - val_loss: 1.5534 - val_accuracy: 0.4840\nEpoch 8/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.3413 - accuracy: 0.5347 - val_loss: 1.5411 - val_accuracy: 0.4806\nEpoch 9/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.3048 - accuracy: 0.5471 - val_loss: 1.6424 - val_accuracy: 0.4978\nEpoch 10/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.2677 - accuracy: 0.5614 - val_loss: 1.5810 - val_accuracy: 0.4996\nEpoch 11/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.2346 - accuracy: 0.5763 - val_loss: 1.6850 - val_accuracy: 0.4960\nEpoch 12/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.2084 - accuracy: 0.5830 - val_loss: 1.5737 - val_accuracy: 0.5166\nEpoch 13/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.1808 - accuracy: 0.5945 - val_loss: 1.5958 - val_accuracy: 0.5036\nEpoch 14/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.1531 - accuracy: 0.6044 - val_loss: 1.5917 - val_accuracy: 0.5166\nEpoch 15/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.1280 - accuracy: 0.6158 - val_loss: 1.7426 - val_accuracy: 0.5106\nEpoch 16/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.1065 - accuracy: 0.6234 - val_loss: 1.6777 - val_accuracy: 0.5060\nEpoch 17/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 1.0803 - accuracy: 0.6284 - val_loss: 1.6933 - val_accuracy: 0.5212\nEpoch 18/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.0512 - accuracy: 0.6416 - val_loss: 1.7519 - val_accuracy: 0.5142\nEpoch 19/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.0339 - accuracy: 0.6454 - val_loss: 1.7969 - val_accuracy: 0.5064\nEpoch 20/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 1.0110 - accuracy: 0.6549 - val_loss: 1.7003 - val_accuracy: 0.5272\nEpoch 21/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 0.9943 - accuracy: 0.6642 - val_loss: 1.8760 - val_accuracy: 0.5172\nEpoch 22/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 0.9734 - accuracy: 0.6689 - val_loss: 1.7995 - val_accuracy: 0.5162\nEpoch 23/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 0.9482 - accuracy: 0.6760 - val_loss: 1.8466 - val_accuracy: 0.5112\nEpoch 24/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 0.9356 - accuracy: 0.6833 - val_loss: 1.8378 - val_accuracy: 0.5118\nEpoch 25/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 0.9199 - accuracy: 0.6869 - val_loss: 1.9511 - val_accuracy: 0.5144\nEpoch 26/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 0.8992 - accuracy: 0.6938 - val_loss: 1.8396 - val_accuracy: 0.5160\nEpoch 27/100\n1407/1407 [==============================] - 13s 10ms/step - loss: 0.8791 - accuracy: 0.7029 - val_loss: 2.1856 - val_accuracy: 0.5230\nEpoch 28/100\n1407/1407 [==============================] - 14s 10ms/step - loss: 0.8723 - accuracy: 0.7052 - val_loss: 1.9413 - val_accuracy: 0.5070\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fe200de0a50>"},"metadata":{}}]},{"cell_type":"code","source":"model = keras.models.load_model(\"cifar10_selu_alpha.h5\")\noptimizer = keras.optimizers.Nadam(lr=5e-4)\nmodel.compile(loss='sparse_categorical_crossentropy',\n             optimizer=optimizer,\n             metrics=['accuracy'])\nmodel.evaluate(X_valid_scaled, y_valid, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T19:25:20.205273Z","iopub.execute_input":"2021-06-08T19:25:20.205625Z","iopub.status.idle":"2021-06-08T19:25:21.262136Z","shell.execute_reply.started":"2021-06-08T19:25:20.205593Z","shell.execute_reply":"2021-06-08T19:25:21.261417Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[1.5410923957824707, 0.4805999994277954]"},"metadata":{}}]},{"cell_type":"markdown","source":"### 1cycle policy","metadata":{}},{"cell_type":"code","source":"# callback to schedule learning rate exponentially \nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs[\"loss\"])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n\n# Learning rate finder\ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = len(X) // batch_size * epochs\n    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n    init_lr = K.get_value(model.optimizer.lr)\n    K.set_value(model.optimizer.lr, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n                        callbacks=[exp_lr])\n    K.set_value(model.optimizer.lr, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n\n# log-LR / loss plot \ndef plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n    plt.xlabel(\"Learning rate\")\n    plt.ylabel(\"Loss\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:29:52.090518Z","iopub.execute_input":"2021-06-09T15:29:52.090847Z","iopub.status.idle":"2021-06-09T15:29:52.104556Z","shell.execute_reply.started":"2021-06-09T15:29:52.090815Z","shell.execute_reply":"2021-06-09T15:29:52.103719Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# define model\ndef get_model(drop_rate):\n    keras.backend.clear_session()\n    tf.random.set_seed(42)\n    np.random.seed(42)\n\n    model = keras.models.Sequential()\n    model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n    for _ in range(20):\n        model.add(keras.layers.Dense(100,\n                                     kernel_initializer=\"lecun_normal\",\n                                     activation=\"selu\"))\n\n    model.add(keras.layers.AlphaDropout(rate=drop_rate))\n    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n\n    optimizer = keras.optimizers.SGD(lr=1e-3, momentum=0.9)\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n                  optimizer=optimizer,\n                  metrics=[\"accuracy\"])\n    return model\n\nmodel = get_model(drop_rate=0.25)\nbatch_size = 128\n# get rates and losses\nrates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n# rate/loss plot \nplot_lr_vs_loss(rates, losses)\nplt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:41:54.384216Z","iopub.execute_input":"2021-06-09T15:41:54.384532Z","iopub.status.idle":"2021-06-09T15:41:58.006501Z","shell.execute_reply.started":"2021-06-09T15:41:54.384504Z","shell.execute_reply":"2021-06-09T15:41:58.005754Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"352/352 [==============================] - 2s 4ms/step - loss: nan - accuracy: 0.1518\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(9.999999747378752e-06,\n 9.999868392944336,\n 2.5100045204162598,\n 3.9331347601754327)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"class OneCycleScheduler(keras.callbacks.Callback):\n    def __init__(self, iterations, max_rate, start_rate=None,\n                 last_iterations=None, last_rate=None):\n        self.iterations = iterations\n        self.max_rate = max_rate\n        self.max_momentum = 0.95\n        self.min_momentum = 0.85\n        self.last_momentum = 0.95\n        self.start_rate = start_rate or max_rate / 10\n        self.last_iterations = last_iterations or iterations // 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) // 2\n        self.last_rate = last_rate or self.start_rate / 1000\n        self.iteration = 0\n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return ((rate2 - rate1) * (self.iteration - iter1)\n                / (iter2 - iter1) + rate1)\n    def on_batch_begin(self, batch, logs):\n        if self.iteration < self.half_iteration:\n            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n            momentum = self._interpolate(0, self.half_iteration, self.max_momentum, self.min_momentum)\n        elif self.iteration < 2 * self.half_iteration:\n            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.max_rate, self.start_rate)\n            momentum = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.min_momentum, self.max_momentum)\n        else:\n            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.start_rate, self.last_rate)\n            rate = max(rate, self.last_rate)\n            momentum = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.max_momentum, self.min_momentum)\n            momentum = max(momentum, self.max_momentum)\n        self.iteration += 1\n        K.set_value(self.model.optimizer.lr, rate)\n        K.set_value(self.model.optimizer.momentum, momentum)\n\nmodel = get_model(drop_rate=0.2)\nn_epochs = 15\nonecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=0.05)\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n                    validation_data=(X_valid_scaled, y_valid),\n                    callbacks=[onecycle])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:43:09.459698Z","iopub.execute_input":"2021-06-09T15:43:09.460009Z","iopub.status.idle":"2021-06-09T15:43:37.620856Z","shell.execute_reply.started":"2021-06-09T15:43:09.459982Z","shell.execute_reply":"2021-06-09T15:43:37.620086Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch 1/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.9550 - accuracy: 0.3024 - val_loss: 1.9487 - val_accuracy: 0.3622\nEpoch 2/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.7458 - accuracy: 0.3811 - val_loss: 1.8067 - val_accuracy: 0.4142\nEpoch 3/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.6632 - accuracy: 0.4159 - val_loss: 1.7410 - val_accuracy: 0.4268\nEpoch 4/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.6137 - accuracy: 0.4320 - val_loss: 1.7015 - val_accuracy: 0.4236\nEpoch 5/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.5691 - accuracy: 0.4523 - val_loss: 1.6533 - val_accuracy: 0.4486\nEpoch 6/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.5334 - accuracy: 0.4647 - val_loss: 1.6567 - val_accuracy: 0.4648\nEpoch 7/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.4983 - accuracy: 0.4759 - val_loss: 1.6867 - val_accuracy: 0.4530\nEpoch 8/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.4670 - accuracy: 0.4900 - val_loss: 1.6942 - val_accuracy: 0.4514\nEpoch 9/15\n352/352 [==============================] - 2s 6ms/step - loss: 1.4154 - accuracy: 0.5079 - val_loss: 1.7209 - val_accuracy: 0.4728\nEpoch 10/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.3654 - accuracy: 0.5276 - val_loss: 1.6059 - val_accuracy: 0.4924\nEpoch 11/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.3037 - accuracy: 0.5518 - val_loss: 1.5864 - val_accuracy: 0.5046\nEpoch 12/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.2512 - accuracy: 0.5673 - val_loss: 1.6403 - val_accuracy: 0.5100\nEpoch 13/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.1677 - accuracy: 0.5977 - val_loss: 1.5784 - val_accuracy: 0.5154\nEpoch 14/15\n352/352 [==============================] - 2s 5ms/step - loss: 1.0605 - accuracy: 0.6310 - val_loss: 1.5562 - val_accuracy: 0.5292\nEpoch 15/15\n352/352 [==============================] - 2s 5ms/step - loss: 0.9534 - accuracy: 0.6684 - val_loss: 1.6171 - val_accuracy: 0.5382\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(X_valid_scaled, y_valid)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T15:43:52.636524Z","iopub.execute_input":"2021-06-09T15:43:52.636863Z","iopub.status.idle":"2021-06-09T15:43:53.097691Z","shell.execute_reply.started":"2021-06-09T15:43:52.636827Z","shell.execute_reply":"2021-06-09T15:43:53.097007Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 0s 2ms/step - loss: 1.6171 - accuracy: 0.5382\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[1.6171127557754517, 0.5382000207901001]"},"metadata":{}}]},{"cell_type":"code","source":"# MCAlphaDropout \nclass MCAlphaDropout(keras.layers.AlphaDropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n\n# replace AlphaDropout layers with MCAlphaDropout\nmc_model = keras.models.Sequential([\n    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n    for layer in model.layers\n])\n\n# define function to get class probabilities for each sample in data\ndef mc_dropout_predict_probas(mc_model, X, n_samples):\n    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n    return np.mean(Y_probas, axis=0)\n\n# define function to predict class based on maximum probability \ndef mc_dropout_predict_classes(mc_model, X, n_samples):\n    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n    return np.argmax(Y_probas, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:06:32.811225Z","iopub.execute_input":"2021-06-09T16:06:32.811552Z","iopub.status.idle":"2021-06-09T16:06:32.900235Z","shell.execute_reply.started":"2021-06-09T16:06:32.811522Z","shell.execute_reply":"2021-06-09T16:06:32.899329Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# compare different settings of n_samples hyperparameter for MCDropout inference\nfor n_samples in [10, 50, 100, 150, 200]:\n    start = timeit.default_timer()\n    y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled, n_samples)\n    accuracy = np.mean(y_pred == y_valid[:, 0])\n    print(\"MCDropout {}-sample accuracy is {}. Time taken: {} seconds.\".format(n_samples, accuracy, round(timeit.default_timer() - start)))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:02:56.262971Z","iopub.execute_input":"2021-06-09T16:02:56.263277Z","iopub.status.idle":"2021-06-09T16:05:40.962616Z","shell.execute_reply.started":"2021-06-09T16:02:56.263249Z","shell.execute_reply":"2021-06-09T16:05:40.961728Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"MCDropout 10-sample accuracy is 0.5396. Time taken: 3 seconds.\nMCDropout 50-sample accuracy is 0.5392. Time taken: 16 seconds.\nMCDropout 100-sample accuracy is 0.5406. Time taken: 33 seconds.\nMCDropout 150-sample accuracy is 0.5396. Time taken: 48 seconds.\nMCDropout 200-sample accuracy is 0.5394. Time taken: 65 seconds.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}